.row.mt-4.mb-2
  h2 #[span.emoji ðŸš€] Upcoming

.row.next
  .col-md-7.col-8
    p.author #[span.me Michele Papucci]
    h2.title.mb-0.mt-1
      | Toronto is the capital of Canada: Detecting and Preventing LLMs from Hallucinating
  .col-md-2.col-4
    h1.day 7th
    h4.month.mb-0 February
    p 15:00-16:00
  .col-md-7.col-12.mt-sm-3.mt-lg-1
      p
        | Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) applications, reaching new state-of-the-art performances in all kinds of tasks and domains, especially in content generation scenarios. However, these models are prone to generating Hallucinations, which are factually incorrect, misleading, and non-sensical outputs that are presented in a convincingly accurate manner. This phenomenon poses significant risks, particularly in fields such as healthcare, legal services, etc. This seminar will briefly present the nature of hallucinations in LLMs, their underlying causes, and the challenges in detecting them. By understanding and preventing hallucination, LLM reliability can be improved, ensuring safer and more trustworthy AI applications.
  .col-md-4.d-grid.gap-2.d-md-block
    
    a(href="https://teams.microsoft.com/l/meetup-join/19%3aDkLh2QJfJIzQp_SpEuyUvxNI3ktQUEzrE2Aob9rjKVE1%40thread.tacv2/1738577547426?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22919a988f-192a-44fb-85ea-da1c780ea337%22%7d").btn.btn-primary.mb-md-3.w-100
      | #[i.bi.bi-camera-reels-fill] Live Streaming
    a(href="https://goo.gl/maps/FL4qcbB3MnMXrYS28",target="_blank").btn.btn-primary.w-100
      | #[i.bi.bi-geo-alt-fill] Sala Riunioni Est
      