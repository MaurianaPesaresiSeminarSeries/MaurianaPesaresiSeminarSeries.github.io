.row.mt-4.mb-4
  h2 #[span.emoji ðŸ”®] Next Talks

.row.project
  .col-md-2.col-3
    h3.mb-0 16th
    h5.month.mb-0 February
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Molo Mbasa Joaquim]
    h4.title.mb-1
      | Towards Knowledge Distillation in Decentralized Learning for Edge AI. Challenges and Future Directions
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-0762599efc47fbbdc00297189827fb14",aria-expanded="false",aria-controls="talk-0762599efc47fbbdc00297189827fb14")
        | #[i.bi.bi-file-earmark-text-fill] Abstract
      a(href="slides/Towards Knowledge Distillation in Decentralized Learning for Edge AI. Challenges and Future Directions.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-easel3-fill] Slides
  .col-md-7
    p.abstract#talk-0762599efc47fbbdc00297189827fb14.collapse.mt-2
        | Recent years have witnessed significant advancement in Artificial Intelligence (AI), particularly with the rise of Deep Neural Networks (DNNs) fueled by large datasets and increased model complexity. However, the demand for substantial computational resources poses challenges, especially in decentralized data scenarios. Edge Intelligence (EI), combining Edge Computing (EC) and AI, emerges as a transformative solution for decentralized learning, crucial in the era of IoT proliferation. Federated Learning (FL) and Knowledge Distillation (KD) represent prominent paradigms in decentralized learning, each with its unique challenges and opportunities. This seminar delves into the development of KD as a decentralized learning method, exploring its principles, unresolved challenges, and promising research avenues.