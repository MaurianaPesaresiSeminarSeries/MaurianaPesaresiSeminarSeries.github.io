Date,Name,Title,Abstract,Room,Hours,Meet
24/1/2025,Calogero Turco,Trust Approaches in Self-Sovereign Identity,"Digital Identity today is mainly centralized, with platforms like Google and Facebook controlling user's data. This lack of control has led to significant privacy concerns, such as the Cambridge Analytica scandal. Self-Sovereign Identity (SSI) is a paradigm shift that decentralizes Digital Identity by giving control back to the individuals it refers to. In SSI, individuals are called Holders and can decide what parts of their identity to expose. Each Holder has Verifiable Credentials (e.g., degree) in their digital wallet, issued by entities called Issuers (e.g., University). Holders present those Verifiable Credentials to any subject interested in information about them and who can act as a Verifier (e.g., recruiter). Unfortunately, no consolidated solutions exist to certify the Issuer's trustworthiness in issuing a specific credential or that the Holder can trust the Verifier in managing their sensitive data. How do we ensure that Issuers are trustworthy and Verifiers are competent to handle sensitive data responsibly? We will see some proposed methods for this and evaluate their suitability for different scenarios. Resolving these issues is essential to fully realizing the potential of SSI and creating a more secure, user-centric Digital Identity ecosystem.",Sala Riunioni Est,15:00-16:00,https://teams.microsoft.com/l/message/19:DkLh2QJfJIzQp_SpEuyUvxNI3ktQUEzrE2Aob9rjKVE1@thread.tacv2/1737121418482?tenantId=c7456b31-a220-47f5-be52-473828670aa1&groupId=e1ec02a8-5e74-4f1f-a340-920ec77b16bf&parentMessageId=1737121418482&teamName=40th%20CS%20Ph.D.%20cycle%20Pesaresi%20Seminar%20Series&channelName=General&createdTime=1737121418482,
07/2/2025,Michele Papucci,"Toronto is the capital of Canada": Detecting and Preventing LLMs from Hallucinating,"Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) applications, reaching new state-of-the-art performances in all kinds of tasks and domains, especially in content generation scenarios. However, these models are prone to generating Hallucinations, which are factually incorrect, misleading, and non-sensical outputs that are presented in a convincingly accurate manner. This phenomenon poses significant risks, particularly in fields such as healthcare, legal services, etc. This seminar will briefly present the nature of hallucinations in LLMs, their underlying causes, and the challenges in detecting them. By understanding and preventing hallucination, LLM reliability can be improved, ensuring safer and more trustworthy AI applications.", Sala Riunioni Est,15:00-16:00,https://teams.microsoft.com/l/meetup-join/19%3aDkLh2QJfJIzQp_SpEuyUvxNI3ktQUEzrE2Aob9rjKVE1%40thread.tacv2/1738577547426?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22919a988f-192a-44fb-85ea-da1c780ea337%22%7d